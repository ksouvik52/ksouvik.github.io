+++
title = "Projects"
description = "Research Projects"
date = "2020-11-03"
aliases = ["projects", "research-pprojects", "research"]
author = "Souvik Kundu"
+++


#### 1. Reducing Spiking Activity of SNNs via Brain-Inspired Learning 
We propose an attention-guided compression scheme yield extremely energy-efficient SNNs. In particular, we provide a 2 stage training strategy to compress SNNs to yield extremely energy-efficient models that requires one order of lower energy compared to iso-parameter ANNs.
{{< rawhtml >}}
<img src="/images/wacv.jpg" alt="drawing" width="450"/>
{{< /rawhtml >}}

(*Accepted at WACV 2021*)

#### 2. Making Compressed Models Adversarially Robust
We propose a one-shot training framework to generate robust yet compressed DNN models. In particular, the proposed robust sparse learning strategy can achieve up to 20x compression with negligible drop in both clean and purturbed image accuracy.
{{< rawhtml >}}
<img src="/images/asp_dac2021.jpg" alt="drawing" width="450"/>
{{< /rawhtml >}}

(*Accepetd at ASP-DAC 2021*)

#### 3. Are Dense CNN Kernels Really Necessary?
A study of sparsely represented kernels, where we propose hardware friendly sparse models enabling low cost data-transfer from DRAM. We further extend the work to propose periodically repeating sparse kernels and periodic compressed sparse representation styles to reduce the data transfer need.

{{< rawhtml >}}
<img src="/images/IEEE_TC.jpg" alt="drawing" width="200"/>
{{< /rawhtml >}}

(*IEEE Trans. on Computers 2020, Allerton 2019, Best poster at USC research festival 2019*)

#### 4. Making MCA Based Crossbar Array More Compact
We propose a new form of pre-defined sparsity that can reduce the cross bar array size drastically, thus helps in making RRAM based ML accelerators
{{< rawhtml >}}
<img src="/images/isvlsi2019.jpg" alt="drawing" width="250"/>
{{< /rawhtml >}}

(*ISVLSI 2019*)
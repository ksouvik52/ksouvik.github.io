+++
title = "Projects"
description = "Research Projects"
date = "2020-11-03"
aliases = ["projects", "research-pprojects", "research"]
author = "Souvik Kundu"
+++

#### 1. Analysis of Model IP Vulnerability for Undistillable Models
We thoroughly analyze the model IP vulnerability of models used in machine learning as a service applications (MLAAS). We further propose a distillation framework using a novel **Skeptical student** that can distill knowledge from even the undistillable models.

{{< rawhtml >}}
<img src="/images/neurips2021_framework.jpg" alt="drawing" width="450"/>
{{< /rawhtml >}}

(*Accepted at NeurIPS 2021 {{< rawhtml >}} <span style="color:blue">[h5 index 245]</span>{{< /rawhtml >}}*)

#### 2. A Novel Training Algorithm to Improve Model Robustness of Deep SNNs 
We propose a SNN training strategy that can yield improved model robustness without causing any significant sacrifice in clean image classification performance, that **too at reduced memory budget and no extra training time**.

{{< rawhtml >}}
<img src="/images/iccv2021.jpg" alt="drawing" width="450"/>
{{< /rawhtml >}}

(*Accepted at ICCV 2021 {{< rawhtml >}} <span style="color:blue">[h5 index 184]</span>{{< /rawhtml >}}*)


#### 3. Reducing Spiking Activity of SNNs via Brain-Inspired Learning 
We propose an attention-guided compression scheme yield extremely energy-efficient SNNs. In particular, we provide a 2 stage training strategy to compress SNNs to yield extremely energy-efficient models that requires one order of lower energy compared to iso-parameter ANNs.
{{< rawhtml >}}
<img src="/images/wacv.jpg" alt="drawing" width="450"/>
{{< /rawhtml >}}

(*Accepted at WACV 2021*)

#### 4. Making Compressed Models Adversarially Robust
We propose a one-shot training framework to generate robust yet compressed DNN models. In particular, the proposed robust sparse learning strategy can achieve up to 20x compression with negligible drop in both clean and purturbed image accuracy.
{{< rawhtml >}}
<img src="/images/asp_dac2021.jpg" alt="drawing" width="450"/>
{{< /rawhtml >}}

(*Accepetd at ASP-DAC 2021*)

#### 5. Are Dense CNN Kernels Really Necessary?
A study of sparsely represented kernels, where we propose hardware friendly sparse models enabling low cost data-transfer from DRAM. We further extend the work to propose periodically repeating sparse kernels and periodic compressed sparse representation styles to reduce the data transfer need.

{{< rawhtml >}}
<img src="/images/IEEE_TC.jpg" alt="drawing" width="200"/>
{{< /rawhtml >}}

(*IEEE Trans. on Computers 2020, Allerton 2019, Best poster at USC research festival 2019*)

#### 6. Making MCA Based Crossbar Array More Compact
We propose a new form of pre-defined sparsity that can reduce the cross bar array size drastically, thus helps in making RRAM based ML accelerators
{{< rawhtml >}}
<img src="/images/isvlsi2019.jpg" alt="drawing" width="250"/>
{{< /rawhtml >}}

(*Accepetd at ISVLSI 2019*)